[tool.poetry]
name = "followbench-evaluation"
version = "0.1.0"
description = "A comprehensive toolkit for evaluating LLMs on instruction-following capabilities using the FollowBench methodology"
authors = ["Your Name <your.email@example.com>"]
readme = "README.md"
license = "MIT"
packages = [{include = "src"}]

[tool.poetry.dependencies]
python = "^3.11,<3.14"
openai = "^1.0.0"
pandas = "^2.0.0"
numpy = "^1.24.0"
datasets = "^2.0.0"
python-dotenv = "^1.0.0"

[tool.poetry.group.dev.dependencies]
pytest = "^7.0.0"
black = "^23.0.0"
flake8 = "^6.0.0"
mypy = "^1.0.0"

[tool.poetry.scripts]
followbench = "src.pipeline:main"
run-followbench = "src.run_followbench:main"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.black]
line-length = 100
target-version = ['py310']

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
ignore_missing_imports = true
